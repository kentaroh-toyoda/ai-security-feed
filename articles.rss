<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>AI Security Digest</title>
    <link>https://kentaroh-toyoda.github.io/ai-security-feed</link>
    <description>Curated AI security insights and articles from various sources</description>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>AI Security Digest Agent</generator>
    <language>en</language>
    <lastBuildDate>Mon, 29 Sep 2025 21:36:00 +0000</lastBuildDate>
    <item>
      <title>TAU-bench extension: benchmarking policy-aware agents in realistic settings</title>
      <link>https://toloka.ai/tau-bench-extension-benchmarking-policy-aware-agents-in-realistic-settings/</link>
      <description>&lt;div style="border: 1px solid #ddd; border-radius: 8px; padding: 12px; margin: 8px 0; background-color: #f9f9f9;"&gt;
&lt;div style="font-weight: bold; color: #2c3e50; margin-bottom: 8px;"&gt;
üìÑ Source: Toloka Blog
&lt;/div&gt;
&lt;div style="font-size: 0.9em; margin-bottom: 8px;"&gt;
&lt;span style="color: #7f8c8d;"&gt;üìÖ Date: Sep 24, 2025&lt;/span&gt;
&lt;/div&gt;
&lt;div style="margin-bottom: 8px;"&gt;
&lt;span style="background-color: #3498db; color: white; padding: 2px 6px; border-radius: 3px; font-size: 0.8em;"&gt;Artificial Intelligence&lt;/span&gt; &lt;span style="background-color: #3498db; color: white; padding: 2px 6px; border-radius: 3px; font-size: 0.8em;"&gt;Reinforcement Learning&lt;/span&gt; &lt;span style="background-color: #3498db; color: white; padding: 2px 6px; border-radius: 3px; font-size: 0.8em;"&gt;Agent Benchmarking&lt;/span&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div style="line-height: 1.6;"&gt;
The article introduces an extension of TAU-bench aimed at benchmarking policy-aware agents within realistic environments. This extension offers enhanced insights into the performance and behavior of such agents under practical conditions.
&lt;/div&gt;</description>
      <guid isPermaLink="false">https://toloka.ai/tau-bench-extension-benchmarking-policy-aware-agents-in-realistic-settings/</guid>
      <category>Artificial Intelligence</category>
      <category>Reinforcement Learning</category>
      <category>Agent Benchmarking</category>
      <category>Machine Learning Evaluation</category>
      <category>Policy-aware Systems</category>
      <pubDate>Mon, 29 Sep 2025 21:36:00 +0000</pubDate>
      <source url="https://toloka.ai/blog">Custom Web Page</source>
    </item>
    <item>
      <title>The AI Agents That Trust Too Much: Are Your Agentic Workflows Vulnerable To Attack? PART 2</title>
      <link>https://splx.ai/blog/indirect-prompt-injection-agentic-ai-security</link>
      <description>&lt;div style="border: 1px solid #ddd; border-radius: 8px; padding: 12px; margin: 8px 0; background-color: #f9f9f9;"&gt;
&lt;div style="font-weight: bold; color: #2c3e50; margin-bottom: 8px;"&gt;
üìÑ Source: SPLX AI Blog
&lt;/div&gt;
&lt;div style="font-size: 0.9em; margin-bottom: 8px;"&gt;
&lt;span style="color: #7f8c8d;"&gt;üë§ Author: Ante Gojsalic, Andrija Dumanƒçiƒá&lt;/span&gt; | &lt;span style="color: #7f8c8d;"&gt;üìÖ Date: Sep 29, 2025&lt;/span&gt;
&lt;/div&gt;
&lt;div style="margin-bottom: 8px;"&gt;
&lt;span style="background-color: #3498db; color: white; padding: 2px 6px; border-radius: 3px; font-size: 0.8em;"&gt;AI Security&lt;/span&gt; &lt;span style="background-color: #3498db; color: white; padding: 2px 6px; border-radius: 3px; font-size: 0.8em;"&gt;Agentic AI&lt;/span&gt; &lt;span style="background-color: #3498db; color: white; padding: 2px 6px; border-radius: 3px; font-size: 0.8em;"&gt;Data Poisoning&lt;/span&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div style="line-height: 1.6;"&gt;
The article explores how agentic AI workflows can be compromised through poisoned data embedded in seemingly trusted sources like web pages, without any malicious user input. It demonstrates a test case where hidden malicious instructions in raw HTML led to a full system compromise, emphasizing the need for hardened prompts, guardrails, and data sanitization to secure these AI systems. The vulnerability arises from assumptions that external data and internal agent outputs are inherently safe and that instruction-like text is always valid.
&lt;/div&gt;</description>
      <guid isPermaLink="false">https://splx.ai/blog/indirect-prompt-injection-agentic-ai-security</guid>
      <category>AI Security</category>
      <category>Agentic AI</category>
      <category>Data Poisoning</category>
      <category>Cybersecurity</category>
      <category>Prompt Injection</category>
      <pubDate>Mon, 29 Sep 2025 21:36:00 +0000</pubDate>
      <source url="https://splx.ai/blog">Custom Web Page</source>
    </item>
  </channel>
</rss>
