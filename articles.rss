<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>AI Security Digest</title>
    <link>https://kentaroh-toyoda.github.io/ai-security-feed</link>
    <description>Curated AI security insights and articles from various sources</description>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>AI Security Digest Agent</generator>
    <language>en</language>
    <lastBuildDate>Fri, 26 Sep 2025 21:36:00 +0000</lastBuildDate>
    <item>
      <title>Evaluating Explainability in Machine Learning Predictions through Explainer-Agnostic Metrics</title>
      <link>https://arxiv.org/pdf/2302.12094</link>
      <description>&lt;div style="border: 1px solid #ddd; border-radius: 8px; padding: 12px; margin: 8px 0; background-color: #f9f9f9;"&gt;
&lt;div style="font-weight: bold; color: #2c3e50; margin-bottom: 8px;"&gt;
ðŸ“„ Source: Holistic AI Research
&lt;/div&gt;
&lt;div style="font-size: 0.9em; margin-bottom: 8px;"&gt;
&lt;span style="color: #7f8c8d;"&gt;ðŸ“… Date: November 6, 2024&lt;/span&gt;
&lt;/div&gt;
&lt;div style="margin-bottom: 8px;"&gt;
&lt;span style="background-color: #3498db; color: white; padding: 2px 6px; border-radius: 3px; font-size: 0.8em;"&gt;Machine Learning&lt;/span&gt; &lt;span style="background-color: #3498db; color: white; padding: 2px 6px; border-radius: 3px; font-size: 0.8em;"&gt;Explainable AI&lt;/span&gt; &lt;span style="background-color: #3498db; color: white; padding: 2px 6px; border-radius: 3px; font-size: 0.8em;"&gt;Model Evaluation&lt;/span&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div style="line-height: 1.6;"&gt;
The paper presents an evaluation framework for explainability in machine learning predictions using metrics that do not depend on any specific explainer method. This approach aims to provide a standardized way to assess the quality and reliability of explanations across different models and techniques.
&lt;/div&gt;</description>
      <guid isPermaLink="false">https://arxiv.org/pdf/2302.12094</guid>
      <category>Machine Learning</category>
      <category>Explainable AI</category>
      <category>Model Evaluation</category>
      <category>Artificial Intelligence</category>
      <category>Data Science</category>
      <pubDate>Fri, 26 Sep 2025 21:36:00 +0000</pubDate>
      <source url="https://www.holisticai.com/papers">Custom Web Page</source>
    </item>
    <item>
      <title>HEARTS: A Holistic Framework for Explainable, Sustainable and Robust Text Stereotype Detection</title>
      <link>https://arxiv.org/pdf/2409.11579</link>
      <description>&lt;div style="border: 1px solid #ddd; border-radius: 8px; padding: 12px; margin: 8px 0; background-color: #f9f9f9;"&gt;
&lt;div style="font-weight: bold; color: #2c3e50; margin-bottom: 8px;"&gt;
ðŸ“„ Source: Holistic AI Research
&lt;/div&gt;
&lt;div style="font-size: 0.9em; margin-bottom: 8px;"&gt;
&lt;span style="color: #7f8c8d;"&gt;ðŸ“… Date: November 30, 2024&lt;/span&gt;
&lt;/div&gt;
&lt;div style="margin-bottom: 8px;"&gt;
&lt;span style="background-color: #3498db; color: white; padding: 2px 6px; border-radius: 3px; font-size: 0.8em;"&gt;Natural Language Processing&lt;/span&gt; &lt;span style="background-color: #3498db; color: white; padding: 2px 6px; border-radius: 3px; font-size: 0.8em;"&gt;Explainable AI&lt;/span&gt; &lt;span style="background-color: #3498db; color: white; padding: 2px 6px; border-radius: 3px; font-size: 0.8em;"&gt;Bias Detection&lt;/span&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div style="line-height: 1.6;"&gt;
The paper introduces HEARTS, a comprehensive framework designed to detect text stereotypes with an emphasis on explainability, sustainability, and robustness. This approach aims to improve the transparency and reliability of stereotype detection in textual data.
&lt;/div&gt;</description>
      <guid isPermaLink="false">https://arxiv.org/pdf/2409.11579</guid>
      <category>Natural Language Processing</category>
      <category>Explainable AI</category>
      <category>Bias Detection</category>
      <category>Sustainability in AI</category>
      <category>Robust Machine Learning</category>
      <pubDate>Fri, 26 Sep 2025 21:36:00 +0000</pubDate>
      <source url="https://www.holisticai.com/papers">Custom Web Page</source>
    </item>
    <item>
      <title>THaMES: An End-to-End Tool for Hallucination Mitigation and Evaluation in Large Language Models</title>
      <link>https://arxiv.org/pdf/2409.11353</link>
      <description>&lt;div style="border: 1px solid #ddd; border-radius: 8px; padding: 12px; margin: 8px 0; background-color: #f9f9f9;"&gt;
&lt;div style="font-weight: bold; color: #2c3e50; margin-bottom: 8px;"&gt;
ðŸ“„ Source: Holistic AI Research
&lt;/div&gt;
&lt;div style="font-size: 0.9em; margin-bottom: 8px;"&gt;
&lt;span style="color: #7f8c8d;"&gt;ðŸ“… Date: November 30, 2024&lt;/span&gt;
&lt;/div&gt;
&lt;div style="margin-bottom: 8px;"&gt;
&lt;span style="background-color: #3498db; color: white; padding: 2px 6px; border-radius: 3px; font-size: 0.8em;"&gt;Natural Language Processing&lt;/span&gt; &lt;span style="background-color: #3498db; color: white; padding: 2px 6px; border-radius: 3px; font-size: 0.8em;"&gt;Large Language Models&lt;/span&gt; &lt;span style="background-color: #3498db; color: white; padding: 2px 6px; border-radius: 3px; font-size: 0.8em;"&gt;AI Safety&lt;/span&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div style="line-height: 1.6;"&gt;
The paper introduces THaMES, an end-to-end tool developed to both mitigate and evaluate hallucinations in large language models. It aims to improve the reliability and accuracy of LLM outputs by addressing hallucination issues systematically.
&lt;/div&gt;</description>
      <guid isPermaLink="false">https://arxiv.org/pdf/2409.11353</guid>
      <category>Natural Language Processing</category>
      <category>Large Language Models</category>
      <category>AI Safety</category>
      <category>Model Evaluation</category>
      <category>Hallucination Mitigation</category>
      <pubDate>Fri, 26 Sep 2025 21:36:00 +0000</pubDate>
      <source url="https://www.holisticai.com/papers">Custom Web Page</source>
    </item>
    <item>
      <title>From Text to Emoji: How PEFT-Driven Personality Manipulation Unleashes the Emoji Potential in LLMs</title>
      <link>https://arxiv.org/pdf/2409.10245</link>
      <description>&lt;div style="border: 1px solid #ddd; border-radius: 8px; padding: 12px; margin: 8px 0; background-color: #f9f9f9;"&gt;
&lt;div style="font-weight: bold; color: #2c3e50; margin-bottom: 8px;"&gt;
ðŸ“„ Source: Holistic AI Research
&lt;/div&gt;
&lt;div style="font-size: 0.9em; margin-bottom: 8px;"&gt;
&lt;span style="color: #7f8c8d;"&gt;ðŸ“… Date: February 25, 2025&lt;/span&gt;
&lt;/div&gt;
&lt;div style="margin-bottom: 8px;"&gt;
&lt;span style="background-color: #3498db; color: white; padding: 2px 6px; border-radius: 3px; font-size: 0.8em;"&gt;Natural Language Processing&lt;/span&gt; &lt;span style="background-color: #3498db; color: white; padding: 2px 6px; border-radius: 3px; font-size: 0.8em;"&gt;Large Language Models&lt;/span&gt; &lt;span style="background-color: #3498db; color: white; padding: 2px 6px; border-radius: 3px; font-size: 0.8em;"&gt;Parameter-Efficient Fine-Tuning&lt;/span&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div style="line-height: 1.6;"&gt;
The paper investigates how Parameter-Efficient Fine-Tuning (PEFT) techniques can be used to manipulate personality traits in large language models (LLMs), thereby improving their ability to use emojis effectively. This approach aims to enhance the expressiveness and contextual relevance of emoji usage in AI-generated text.
&lt;/div&gt;</description>
      <guid isPermaLink="false">https://arxiv.org/pdf/2409.10245</guid>
      <category>Natural Language Processing</category>
      <category>Large Language Models</category>
      <category>Parameter-Efficient Fine-Tuning</category>
      <category>Emoji Usage</category>
      <category>Personality Modeling</category>
      <pubDate>Fri, 26 Sep 2025 21:36:00 +0000</pubDate>
      <source url="https://www.holisticai.com/papers">Custom Web Page</source>
    </item>
    <item>
      <title>Bias Amplification: Large Language Models as Increasingly Biased Media</title>
      <link>https://arxiv.org/pdf/2410.15234</link>
      <description>&lt;div style="border: 1px solid #ddd; border-radius: 8px; padding: 12px; margin: 8px 0; background-color: #f9f9f9;"&gt;
&lt;div style="font-weight: bold; color: #2c3e50; margin-bottom: 8px;"&gt;
ðŸ“„ Source: Holistic AI Research
&lt;/div&gt;
&lt;div style="font-size: 0.9em; margin-bottom: 8px;"&gt;
&lt;span style="color: #7f8c8d;"&gt;ðŸ“… Date: May 20, 2025&lt;/span&gt;
&lt;/div&gt;
&lt;div style="margin-bottom: 8px;"&gt;
&lt;span style="background-color: #3498db; color: white; padding: 2px 6px; border-radius: 3px; font-size: 0.8em;"&gt;Artificial Intelligence&lt;/span&gt; &lt;span style="background-color: #3498db; color: white; padding: 2px 6px; border-radius: 3px; font-size: 0.8em;"&gt;Bias in AI&lt;/span&gt; &lt;span style="background-color: #3498db; color: white; padding: 2px 6px; border-radius: 3px; font-size: 0.8em;"&gt;Natural Language Processing&lt;/span&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div style="line-height: 1.6;"&gt;
The paper examines how large language models can amplify existing biases present in their training data, effectively functioning as media that increasingly propagate these biases. It highlights the implications of such bias amplification for fairness and ethical AI deployment.
&lt;/div&gt;</description>
      <guid isPermaLink="false">https://arxiv.org/pdf/2410.15234</guid>
      <category>Artificial Intelligence</category>
      <category>Bias in AI</category>
      <category>Natural Language Processing</category>
      <category>Ethics in Technology</category>
      <category>Media Studies</category>
      <pubDate>Fri, 26 Sep 2025 21:36:00 +0000</pubDate>
      <source url="https://www.holisticai.com/papers">Custom Web Page</source>
    </item>
    <item>
      <title>LibVulnWatch: A Deep Assessment Agent System and Leaderboard for Uncovering Hidden Vulnerabilities in Open-Source AI Libraries</title>
      <link>https://arxiv.org/pdf/2505.08842</link>
      <description>&lt;div style="border: 1px solid #ddd; border-radius: 8px; padding: 12px; margin: 8px 0; background-color: #f9f9f9;"&gt;
&lt;div style="font-weight: bold; color: #2c3e50; margin-bottom: 8px;"&gt;
ðŸ“„ Source: Holistic AI Research
&lt;/div&gt;
&lt;div style="font-size: 0.9em; margin-bottom: 8px;"&gt;
&lt;span style="color: #7f8c8d;"&gt;ðŸ‘¤ Author: Zekun Wu; Seonglae Cho; Umar Mohammed; Cristian Munoz; Kleyton Costa; Xin Guan; Theo King; Ze Wang; Emre Kazim; Adriano Koshiyama&lt;/span&gt;
&lt;/div&gt;
&lt;div style="margin-bottom: 8px;"&gt;
&lt;span style="background-color: #3498db; color: white; padding: 2px 6px; border-radius: 3px; font-size: 0.8em;"&gt;AI Security&lt;/span&gt; &lt;span style="background-color: #3498db; color: white; padding: 2px 6px; border-radius: 3px; font-size: 0.8em;"&gt;Open-Source Software&lt;/span&gt; &lt;span style="background-color: #3498db; color: white; padding: 2px 6px; border-radius: 3px; font-size: 0.8em;"&gt;Vulnerability Detection&lt;/span&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div style="line-height: 1.6;"&gt;
The paper presents LibVulnWatch, a deep assessment agent system coupled with a leaderboard aimed at identifying hidden security vulnerabilities within open-source AI libraries. This framework facilitates systematic evaluation and benchmarking to enhance the security of AI software components.
&lt;/div&gt;</description>
      <guid isPermaLink="false">https://arxiv.org/pdf/2505.08842</guid>
      <category>AI Security</category>
      <category>Open-Source Software</category>
      <category>Vulnerability Detection</category>
      <category>Software Assessment</category>
      <category>Machine Learning Libraries</category>
      <pubDate>Fri, 26 Sep 2025 21:36:00 +0000</pubDate>
      <source url="https://www.holisticai.com/papers">Custom Web Page</source>
    </item>
    <item>
      <title>MPF: Aligning and Debiasing Language Models post Deployment via Multi Perspective Fusion</title>
      <link>https://arxiv.org/pdf/2507.02595</link>
      <description>&lt;div style="border: 1px solid #ddd; border-radius: 8px; padding: 12px; margin: 8px 0; background-color: #f9f9f9;"&gt;
&lt;div style="font-weight: bold; color: #2c3e50; margin-bottom: 8px;"&gt;
ðŸ“„ Source: Holistic AI Research
&lt;/div&gt;
&lt;div style="font-size: 0.9em; margin-bottom: 8px;"&gt;
&lt;span style="color: #7f8c8d;"&gt;ðŸ‘¤ Author: Xin Guan; PeiHsin Lin; Zekun Wu; Ze Wang; Ruibo Zhang; Emre Kazim; Adriano Koshiyama&lt;/span&gt;
&lt;/div&gt;
&lt;div style="margin-bottom: 8px;"&gt;
&lt;span style="background-color: #3498db; color: white; padding: 2px 6px; border-radius: 3px; font-size: 0.8em;"&gt;Natural Language Processing&lt;/span&gt; &lt;span style="background-color: #3498db; color: white; padding: 2px 6px; border-radius: 3px; font-size: 0.8em;"&gt;Machine Learning&lt;/span&gt; &lt;span style="background-color: #3498db; color: white; padding: 2px 6px; border-radius: 3px; font-size: 0.8em;"&gt;Bias Mitigation&lt;/span&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div style="line-height: 1.6;"&gt;
The article introduces Multi Perspective Fusion (MPF), a technique designed to align and debias language models after deployment. The authors demonstrate through experiments that MPF effectively improves model alignment and reduces biases inherent in language models. Detailed methodology and results are provided in the original paper.
&lt;/div&gt;</description>
      <guid isPermaLink="false">https://arxiv.org/pdf/2507.02595</guid>
      <category>Natural Language Processing</category>
      <category>Machine Learning</category>
      <category>Bias Mitigation</category>
      <category>Model Alignment</category>
      <category>Post-Deployment Techniques</category>
      <pubDate>Fri, 26 Sep 2025 21:36:00 +0000</pubDate>
      <source url="https://www.holisticai.com/papers">Custom Web Page</source>
    </item>
    <item>
      <title>CorrSteer: Steering Improves Task Performance and Safety in LLMs through Correlation-based Sparse Autoencoder Feature Selection</title>
      <link>https://arxiv.org/pdf/2508.12535</link>
      <description>&lt;div style="border: 1px solid #ddd; border-radius: 8px; padding: 12px; margin: 8px 0; background-color: #f9f9f9;"&gt;
&lt;div style="font-weight: bold; color: #2c3e50; margin-bottom: 8px;"&gt;
ðŸ“„ Source: Holistic AI Research
&lt;/div&gt;
&lt;div style="font-size: 0.9em; margin-bottom: 8px;"&gt;
&lt;span style="color: #7f8c8d;"&gt;ðŸ‘¤ Author: Seonglae Cho; Zekun Wu; Adriano Koshiyama&lt;/span&gt;
&lt;/div&gt;
&lt;div style="margin-bottom: 8px;"&gt;
&lt;span style="background-color: #3498db; color: white; padding: 2px 6px; border-radius: 3px; font-size: 0.8em;"&gt;Large Language Models&lt;/span&gt; &lt;span style="background-color: #3498db; color: white; padding: 2px 6px; border-radius: 3px; font-size: 0.8em;"&gt;Machine Learning&lt;/span&gt; &lt;span style="background-color: #3498db; color: white; padding: 2px 6px; border-radius: 3px; font-size: 0.8em;"&gt;Feature Selection&lt;/span&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div style="line-height: 1.6;"&gt;
The paper presents CorrSteer, a method that enhances task performance and safety in large language models by employing correlation-based sparse autoencoder feature selection. This steering technique selectively focuses on relevant features, improving model reliability and output quality.
&lt;/div&gt;</description>
      <guid isPermaLink="false">https://arxiv.org/pdf/2508.12535</guid>
      <category>Large Language Models</category>
      <category>Machine Learning</category>
      <category>Feature Selection</category>
      <category>Model Safety</category>
      <category>Autoencoders</category>
      <pubDate>Fri, 26 Sep 2025 21:36:00 +0000</pubDate>
      <source url="https://www.holisticai.com/papers">Custom Web Page</source>
    </item>
    <item>
      <title>State of AI Regulations in 2025: Everything you need to know</title>
      <link>https://www.holisticai.com/papers/state-of-ai-regulations-ebook</link>
      <description>&lt;div style="border: 1px solid #ddd; border-radius: 8px; padding: 12px; margin: 8px 0; background-color: #f9f9f9;"&gt;
&lt;div style="font-weight: bold; color: #2c3e50; margin-bottom: 8px;"&gt;
ðŸ“„ Source: Holistic AI Research
&lt;/div&gt;
&lt;div style="font-size: 0.9em; margin-bottom: 8px;"&gt;
&lt;span style="color: #7f8c8d;"&gt;ðŸ“… Date: April 13, 2025&lt;/span&gt;
&lt;/div&gt;
&lt;div style="margin-bottom: 8px;"&gt;
&lt;span style="background-color: #3498db; color: white; padding: 2px 6px; border-radius: 3px; font-size: 0.8em;"&gt;Artificial Intelligence&lt;/span&gt; &lt;span style="background-color: #3498db; color: white; padding: 2px 6px; border-radius: 3px; font-size: 0.8em;"&gt;Regulation and Compliance&lt;/span&gt; &lt;span style="background-color: #3498db; color: white; padding: 2px 6px; border-radius: 3px; font-size: 0.8em;"&gt;Technology Policy&lt;/span&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div style="line-height: 1.6;"&gt;
This eBook offers a detailed overview of global AI regulations in 2025, examining legislative frameworks, governance challenges, and implications for AI development. It serves as a guide for AI professionals, legal experts, and policymakers to understand regulatory trends while emphasizing the balance between innovation, ethics, and compliance.
&lt;/div&gt;</description>
      <guid isPermaLink="false">https://www.holisticai.com/papers/state-of-ai-regulations-ebook</guid>
      <category>Artificial Intelligence</category>
      <category>Regulation and Compliance</category>
      <category>Technology Policy</category>
      <category>AI Governance</category>
      <category>Legal Frameworks</category>
      <pubDate>Fri, 26 Sep 2025 21:36:00 +0000</pubDate>
      <source url="https://www.holisticai.com/papers">Custom Web Page</source>
    </item>
  </channel>
</rss>
