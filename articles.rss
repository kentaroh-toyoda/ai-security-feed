<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>AI Security Digest</title>
    <link>https://kentaroh-toyoda.github.io/ai-security-feed</link>
    <description>Curated AI security insights and articles from various sources</description>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>AI Security Digest Agent</generator>
    <language>en</language>
    <lastBuildDate>Tue, 23 Sep 2025 21:35:49 +0000</lastBuildDate>
    <item>
      <title>MDH: Hybrid Jailbreak Detection Strategy</title>
      <link>https://www.promptfoo.dev/lm-security-db/vuln/mdh-hybrid-jailbreak-detection-strategy-xxxxxxx</link>
      <description>&lt;div style="border: 1px solid #ddd; border-radius: 8px; padding: 12px; margin: 8px 0; background-color: #f9f9f9;"&gt;
&lt;div style="font-weight: bold; color: #2c3e50; margin-bottom: 8px;"&gt;
ðŸ“„ Source: PromptFoo LM Security DB
&lt;/div&gt;
&lt;div style="font-size: 0.9em; margin-bottom: 8px;"&gt;
&lt;span style="color: #7f8c8d;"&gt;ðŸ“… Date: 8/31/2025&lt;/span&gt;
&lt;/div&gt;
&lt;div style="margin-bottom: 8px;"&gt;
&lt;span style="background-color: #3498db; color: white; padding: 2px 6px; border-radius: 3px; font-size: 0.8em;"&gt;AI Security&lt;/span&gt; &lt;span style="background-color: #3498db; color: white; padding: 2px 6px; border-radius: 3px; font-size: 0.8em;"&gt;Large Language Models&lt;/span&gt; &lt;span style="background-color: #3498db; color: white; padding: 2px 6px; border-radius: 3px; font-size: 0.8em;"&gt;Jailbreaking Attacks&lt;/span&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div style="line-height: 1.6;"&gt;
The article discusses vulnerabilities in large language models that support developer roles, highlighting a jailbreaking attack called D-Attack which exploits malicious developer messages to bypass safety measures. It also introduces an advanced variant, DH-CoT, that enhances attack success by combining developer message context with hijacked Chain-of-Thought prompts, particularly against reasoning-optimized models.
&lt;/div&gt;</description>
      <guid isPermaLink="false">https://www.promptfoo.dev/lm-security-db/vuln/mdh-hybrid-jailbreak-detection-strategy-xxxxxxx</guid>
      <category>AI Security</category>
      <category>Large Language Models</category>
      <category>Jailbreaking Attacks</category>
      <category>Prompt Engineering</category>
      <category>Model Alignment</category>
      <pubDate>Tue, 23 Sep 2025 21:35:49 +0000</pubDate>
      <source url="https://www.promptfoo.dev/lm-security-db">Custom Web Page</source>
    </item>
  </channel>
</rss>
